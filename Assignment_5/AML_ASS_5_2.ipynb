{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1808590,
          "sourceType": "datasetVersion",
          "datasetId": 989445
        }
      ],
      "dockerImageVersionId": 30683,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Akash Das (MDS202206)"
      ],
      "metadata": {
        "id": "7mG5IaOh7I_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use kaggle to Access GPU for Model Fine Tuning\n"
      ],
      "metadata": {
        "id": "v9vRQ3QP44Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:10.879984Z",
          "iopub.execute_input": "2024-04-12T19:35:10.880405Z",
          "iopub.status.idle": "2024-04-12T19:35:10.919008Z",
          "shell.execute_reply.started": "2024-04-12T19:35:10.880374Z",
          "shell.execute_reply": "2024-04-12T19:35:10.918055Z"
        },
        "trusted": true,
        "id": "1spLjqnA57NL",
        "outputId": "0df1a8ae-586e-48c5-900d-19c80275dc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/sentiment-analysis-dataset/training.1600000.processed.noemoticon.csv\n/kaggle/input/sentiment-analysis-dataset/train.csv\n/kaggle/input/sentiment-analysis-dataset/testdata.manual.2009.06.14.csv\n/kaggle/input/sentiment-analysis-dataset/test.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Libraries"
      ],
      "metadata": {
        "id": "QNj1a0yc_fSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification,get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModelForSequenceClassification, AutoModel\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import os, sys, random\n",
        "import datetime, time, copy\n",
        "import torch, warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:59.737596Z",
          "iopub.execute_input": "2024-04-12T19:35:59.738277Z",
          "iopub.status.idle": "2024-04-12T19:35:59.751187Z",
          "shell.execute_reply.started": "2024-04-12T19:35:59.738247Z",
          "shell.execute_reply": "2024-04-12T19:35:59.750243Z"
        },
        "trusted": true,
        "id": "FY3I_RA957NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "_xFlo5fX779C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/kaggle/input/sentiment-analysis-dataset/train.csv\",encoding=\"iso-8859-1\")\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:14.542189Z",
          "iopub.execute_input": "2024-04-12T19:35:14.543204Z",
          "iopub.status.idle": "2024-04-12T19:35:14.663248Z",
          "shell.execute_reply.started": "2024-04-12T19:35:14.543169Z",
          "shell.execute_reply": "2024-04-12T19:35:14.662095Z"
        },
        "trusted": true,
        "id": "xClOmYlB57NO",
        "outputId": "3f9ec7cc-a825-4ae9-f3e2-bf42f3e95b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 103,
          "output_type": "execute_result",
          "data": {
            "text/plain": "       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text sentiment Time of Tweet Age of User  \\\n0  I`d have responded, if I were going   neutral       morning        0-20   \n1                             Sooo SAD  negative          noon       21-30   \n2                          bullying me  negative         night       31-45   \n3                       leave me alone  negative       morning       46-60   \n4                        Sons of ****,  negative          noon       60-70   \n\n       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n0  Afghanistan          38928346         652860.0               60  \n1      Albania           2877797          27400.0              105  \n2      Algeria          43851044        2381740.0               18  \n3      Andorra             77265            470.0              164  \n4       Angola          32866272        1246700.0               26  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n      <th>Time of Tweet</th>\n      <th>Age of User</th>\n      <th>Country</th>\n      <th>Population -2020</th>\n      <th>Land Area (Km²)</th>\n      <th>Density (P/Km²)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n      <td>morning</td>\n      <td>0-20</td>\n      <td>Afghanistan</td>\n      <td>38928346</td>\n      <td>652860.0</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>negative</td>\n      <td>noon</td>\n      <td>21-30</td>\n      <td>Albania</td>\n      <td>2877797</td>\n      <td>27400.0</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n      <td>night</td>\n      <td>31-45</td>\n      <td>Algeria</td>\n      <td>43851044</td>\n      <td>2381740.0</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n      <td>morning</td>\n      <td>46-60</td>\n      <td>Andorra</td>\n      <td>77265</td>\n      <td>470.0</td>\n      <td>164</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n      <td>noon</td>\n      <td>60-70</td>\n      <td>Angola</td>\n      <td>32866272</td>\n      <td>1246700.0</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use kaggle GPU for Model Fine Tuning"
      ],
      "metadata": {
        "id": "snwDihAx8Ayf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "print('Using GPU: ', use_gpu)\n",
        "\n",
        "if use_gpu == True:\n",
        "\tdevice = torch.device(\"cuda\")\n",
        "else:\n",
        "\tdevice = torch.device(\"cpu\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:20.390444Z",
          "iopub.execute_input": "2024-04-12T19:35:20.391307Z",
          "iopub.status.idle": "2024-04-12T19:35:20.396422Z",
          "shell.execute_reply.started": "2024-04-12T19:35:20.391274Z",
          "shell.execute_reply": "2024-04-12T19:35:20.395395Z"
        },
        "trusted": true,
        "id": "fgRmvwVD57NO",
        "outputId": "841a132c-da92-4690-d953-36b282e7ea08"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Using GPU:  True\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "8Bf7mCEt8KEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocessing(text):\n",
        "  regex = r'[^\\w\\s]|[\\U0001f600-\\U0001f64f\\U0001f300-\\U0001f5ff\\U0001f680-\\U0001f6ff\\U0001f1e0-\\U0001f1ff]'\n",
        "  text=re.sub(regex,\" \",text)\n",
        "  text=re.sub(\"\\.|\\,|\\/|\\-\",\" \",text)\n",
        "  text=re.sub(\"\\s*\\s\",\" \",text)\n",
        "  return text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:22.948610Z",
          "iopub.execute_input": "2024-04-12T19:35:22.949596Z",
          "iopub.status.idle": "2024-04-12T19:35:22.955377Z",
          "shell.execute_reply.started": "2024-04-12T19:35:22.949554Z",
          "shell.execute_reply": "2024-04-12T19:35:22.954440Z"
        },
        "trusted": true,
        "id": "zYWqbkJ457NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df)):\n",
        "  df.loc[i,\"text\"]=preprocessing(str(df.loc[i,\"text\"]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:25.371761Z",
          "iopub.execute_input": "2024-04-12T19:35:25.372160Z",
          "iopub.status.idle": "2024-04-12T19:35:38.098966Z",
          "shell.execute_reply.started": "2024-04-12T19:35:25.372129Z",
          "shell.execute_reply": "2024-04-12T19:35:38.098094Z"
        },
        "trusted": true,
        "id": "jkIPp9jl57NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sentiment\"]=df[\"sentiment\"].replace({\"neutral\":0,\"positive\":1,\"negative\":2})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:38.100824Z",
          "iopub.execute_input": "2024-04-12T19:35:38.101293Z",
          "iopub.status.idle": "2024-04-12T19:35:38.128811Z",
          "shell.execute_reply.started": "2024-04-12T19:35:38.101259Z",
          "shell.execute_reply": "2024-04-12T19:35:38.128093Z"
        },
        "trusted": true,
        "id": "rvSoZR2H57NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[[\"text\",\"sentiment\"]]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:38.129789Z",
          "iopub.execute_input": "2024-04-12T19:35:38.130101Z",
          "iopub.status.idle": "2024-04-12T19:35:38.135891Z",
          "shell.execute_reply.started": "2024-04-12T19:35:38.130074Z",
          "shell.execute_reply": "2024-04-12T19:35:38.134930Z"
        },
        "trusted": true,
        "id": "C3MPRj_D57NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:38.137892Z",
          "iopub.execute_input": "2024-04-12T19:35:38.138365Z",
          "iopub.status.idle": "2024-04-12T19:35:38.152441Z",
          "shell.execute_reply.started": "2024-04-12T19:35:38.138332Z",
          "shell.execute_reply": "2024-04-12T19:35:38.151584Z"
        },
        "trusted": true,
        "id": "1mqyldVY57NQ",
        "outputId": "5592dd10-d5da-44fe-d634-83d721437e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 109,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                text  sentiment\n0                 I d have responded if I were going          0\n1        Sooo SAD I will miss you here in San Diego           2\n2                            my boss is bullying me           2\n3                      what interview leave me alone          2\n4   Sons of why couldn t they put them on the rel...          2",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I d have responded if I were going</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sooo SAD I will miss you here in San Diego</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>my boss is bullying me</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what interview leave me alone</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sons of why couldn t they put them on the rel...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:35:40.872449Z",
          "iopub.execute_input": "2024-04-12T19:35:40.873074Z",
          "iopub.status.idle": "2024-04-12T19:35:40.880338Z",
          "shell.execute_reply.started": "2024-04-12T19:35:40.873032Z",
          "shell.execute_reply": "2024-04-12T19:35:40.879429Z"
        },
        "trusted": true,
        "id": "8ik8gZEt57NQ",
        "outputId": "5654d9a0-a40e-4c20-db11-01b8d357434d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 110,
          "output_type": "execute_result",
          "data": {
            "text/plain": "sentiment\n0    11118\n1     8582\n2     7781\nName: count, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset into three sets – train, validation, and test."
      ],
      "metadata": {
        "id": "B3dDV0998RAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split train dataset into train, validation and test sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['sentiment'],\n",
        "                                                                    random_state=2024,\n",
        "                                                                    test_size=0.3,\n",
        "                                                                    stratify=df['sentiment'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                                                                random_state=2024,\n",
        "                                                                test_size=0.5,\n",
        "                                                                stratify=temp_labels)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:36:19.096331Z",
          "iopub.execute_input": "2024-04-12T19:36:19.097021Z",
          "iopub.status.idle": "2024-04-12T19:36:19.121295Z",
          "shell.execute_reply.started": "2024-04-12T19:36:19.096992Z",
          "shell.execute_reply": "2024-04-12T19:36:19.120373Z"
        },
        "trusted": true,
        "id": "thn3DeG157NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import BERT Model and BERT Tokenizer"
      ],
      "metadata": {
        "id": "j4sCUmGS83nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True)\n",
        "\n",
        "# output\n",
        "print(sent_id)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:36:21.699572Z",
          "iopub.execute_input": "2024-04-12T19:36:21.700203Z",
          "iopub.status.idle": "2024-04-12T19:36:22.222563Z",
          "shell.execute_reply.started": "2024-04-12T19:36:21.700173Z",
          "shell.execute_reply": "2024-04-12T19:36:22.221505Z"
        },
        "trusted": true,
        "id": "xV0n5ykz57NR",
        "outputId": "8c44c52a-9d34-454c-be99-ba280c2808e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let’s see how this BERT tokenizer works. We will try to encode a couple of sentences using the tokenizer."
      ],
      "metadata": {
        "id": "FVk81vGq8985"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True)\n",
        "\n",
        "# output\n",
        "print(sent_id)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:36:23.232836Z",
          "iopub.execute_input": "2024-04-12T19:36:23.233212Z",
          "iopub.status.idle": "2024-04-12T19:36:23.239494Z",
          "shell.execute_reply.started": "2024-04-12T19:36:23.233183Z",
          "shell.execute_reply": "2024-04-12T19:36:23.238516Z"
        },
        "trusted": true,
        "id": "bHLYs4vv57NR",
        "outputId": "6910743d-badd-4756-f08b-9af2a5bcfa56"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the Sentences"
      ],
      "metadata": {
        "id": "TvAOLrWH9HDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:36:24.045917Z",
          "iopub.execute_input": "2024-04-12T19:36:24.046777Z",
          "iopub.status.idle": "2024-04-12T19:36:24.322251Z",
          "shell.execute_reply.started": "2024-04-12T19:36:24.046744Z",
          "shell.execute_reply": "2024-04-12T19:36:24.321311Z"
        },
        "trusted": true,
        "id": "ifxkP3J-57NR",
        "outputId": "5b3b20c2-c0be-4291-f8db-8ce2ac1bcb9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 116,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<Axes: >"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:36:24.598803Z",
          "iopub.execute_input": "2024-04-12T19:36:24.599487Z",
          "iopub.status.idle": "2024-04-12T19:36:25.880964Z",
          "shell.execute_reply.started": "2024-04-12T19:36:24.599455Z",
          "shell.execute_reply": "2024-04-12T19:36:25.880129Z"
        },
        "trusted": true,
        "id": "bVqGFEKW57NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the integer sequences to tensors."
      ],
      "metadata": {
        "id": "a0xldeqR9PXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:36:25.882404Z",
          "iopub.execute_input": "2024-04-12T19:36:25.882705Z",
          "iopub.status.idle": "2024-04-12T19:36:26.354582Z",
          "shell.execute_reply.started": "2024-04-12T19:36:25.882681Z",
          "shell.execute_reply": "2024-04-12T19:36:26.353705Z"
        },
        "trusted": true,
        "id": "yy3T7ay-57NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we will create dataloaders for both train and validation set"
      ],
      "metadata": {
        "id": "xaJrSo8P9WHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Warping, sampling the training data and creating dataLoader object\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Warping, sampling the validation data and creating dataLoader object\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:36:27.463972Z",
          "iopub.execute_input": "2024-04-12T19:36:27.464783Z",
          "iopub.status.idle": "2024-04-12T19:36:27.470138Z",
          "shell.execute_reply.started": "2024-04-12T19:36:27.464752Z",
          "shell.execute_reply": "2024-04-12T19:36:27.469274Z"
        },
        "trusted": true,
        "id": "EW_7YK8P57NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model Architecture"
      ],
      "metadata": {
        "id": "z6Ibm3CC9dfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--3 for pos/neu/neg classification.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:36:28.982771Z",
          "iopub.execute_input": "2024-04-12T19:36:28.983428Z",
          "iopub.status.idle": "2024-04-12T19:36:29.572937Z",
          "shell.execute_reply.started": "2024-04-12T19:36:28.983396Z",
          "shell.execute_reply": "2024-04-12T19:36:29.572025Z"
        },
        "trusted": true,
        "id": "95Ojl5BY57NS",
        "outputId": "b7032dba-25c9-493b-ee4b-6992904ed286"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pass the pre-trained BERT to our define architecture\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:37:35.746876Z",
          "iopub.execute_input": "2024-04-12T19:37:35.747634Z",
          "iopub.status.idle": "2024-04-12T19:37:35.760182Z",
          "shell.execute_reply.started": "2024-04-12T19:37:35.747599Z",
          "shell.execute_reply": "2024-04-12T19:37:35.759209Z"
        },
        "trusted": true,
        "id": "gkX4L9sN57NS",
        "outputId": "258b3372-eb7c-494c-ce2c-32002af6dfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:37:38.102919Z",
          "iopub.execute_input": "2024-04-12T19:37:38.103755Z",
          "iopub.status.idle": "2024-04-12T19:37:38.108788Z",
          "shell.execute_reply.started": "2024-04-12T19:37:38.103724Z",
          "shell.execute_reply": "2024-04-12T19:37:38.107981Z"
        },
        "trusted": true,
        "id": "pYNowN-I57NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### use AdamW as our optimizer."
      ],
      "metadata": {
        "id": "1lqmVWHN9sdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-5)          # learning rate\n",
        "\n",
        "\n",
        "# # Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# # 'W' stands for 'Weight Decay fix\"\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:37:41.059712Z",
          "iopub.execute_input": "2024-04-12T19:37:41.060419Z",
          "iopub.status.idle": "2024-04-12T19:37:41.070604Z",
          "shell.execute_reply.started": "2024-04-12T19:37:41.060388Z",
          "shell.execute_reply": "2024-04-12T19:37:41.069654Z"
        },
        "trusted": true,
        "id": "ljPEgxAM57NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There is a class imbalance in our dataset. The majority of the observations are neutral. So, we will first compute class weights for the labels in the train set and then pass these weights to the loss function so that it takes care of the class imbalance."
      ],
      "metadata": {
        "id": "SyCE10yy906s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Compute the class weights\n",
        "# class_labels = np.unique(train_labels)\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels\n",
        "                                    )\n",
        "\n",
        "print(\"Class Weights:\", class_weights)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:37:45.258852Z",
          "iopub.execute_input": "2024-04-12T19:37:45.259468Z",
          "iopub.status.idle": "2024-04-12T19:37:45.270207Z",
          "shell.execute_reply.started": "2024-04-12T19:37:45.259437Z",
          "shell.execute_reply": "2024-04-12T19:37:45.269223Z"
        },
        "trusted": true,
        "id": "LKKdpHaw57NT",
        "outputId": "27fdf9d1-22fd-483d-91b1-fe0704d0983a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Class Weights: [0.82395271 1.06742134 1.17716174]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights)\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:37:47.263689Z",
          "iopub.execute_input": "2024-04-12T19:37:47.264567Z",
          "iopub.status.idle": "2024-04-12T19:37:47.269425Z",
          "shell.execute_reply.started": "2024-04-12T19:37:47.264534Z",
          "shell.execute_reply": "2024-04-12T19:37:47.268583Z"
        },
        "trusted": true,
        "id": "mAZub_U_57NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Required Functions to Fine-Tune BERT\n",
        "\n",
        "So, till now we have defined the model architecture, we have specified the optimizer and the loss function, and our dataloaders are also ready. Now we have to define a couple of functions to train (fine-tune) and evaluate the model, respectively"
      ],
      "metadata": {
        "id": "Tfku9ap4-GEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:37:49.035867Z",
          "iopub.execute_input": "2024-04-12T19:37:49.036237Z",
          "iopub.status.idle": "2024-04-12T19:37:49.042949Z",
          "shell.execute_reply.started": "2024-04-12T19:37:49.036208Z",
          "shell.execute_reply": "2024-04-12T19:37:49.041988Z"
        },
        "trusted": true,
        "id": "rPgp_bs357NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    # preds = model(sent_id, mask)\n",
        "\n",
        "    # # compute the loss between actual and predicted values\n",
        "    # loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # # add on to the total loss\n",
        "    # total_loss = total_loss + loss.item()\n",
        "\n",
        "    output = model(sent_id,\n",
        "                      token_type_ids=None,\n",
        "                      attention_mask=mask,\n",
        "                      labels=labels)\n",
        "\n",
        "    loss, logits = output.loss, output.logits\n",
        "    preds = logits\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:37:50.734161Z",
          "iopub.execute_input": "2024-04-12T19:37:50.734873Z",
          "iopub.status.idle": "2024-04-12T19:37:50.744734Z",
          "shell.execute_reply.started": "2024-04-12T19:37:50.734844Z",
          "shell.execute_reply": "2024-04-12T19:37:50.743876Z"
        },
        "trusted": true,
        "id": "2CflIEzl57NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will use the following function to evaluate the model. It will use the validation set data"
      ],
      "metadata": {
        "id": "nlNcB6TX-WaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "\n",
        "  print(\"\\nEvaluating...\")\n",
        "\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "  t0 = time.time()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "\n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "\n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "\n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # # model predictions\n",
        "      # preds = model(sent_id, mask)\n",
        "\n",
        "      # # compute the validation loss between actual and predicted values\n",
        "      # loss = cross_entropy(preds,labels)\n",
        "\n",
        "      output = model(sent_id,\n",
        "                      token_type_ids=None,\n",
        "                      attention_mask=mask,\n",
        "                      labels=labels)\n",
        "\n",
        "      loss, logits = output.loss, output.logits\n",
        "      preds = logits\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:44:00.433678Z",
          "iopub.execute_input": "2024-04-12T19:44:00.434601Z",
          "iopub.status.idle": "2024-04-12T19:44:00.445144Z",
          "shell.execute_reply.started": "2024-04-12T19:44:00.434569Z",
          "shell.execute_reply": "2024-04-12T19:44:00.444106Z"
        },
        "trusted": true,
        "id": "Uo3fNVnM57NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we will finally start fine-tuning of the model."
      ],
      "metadata": {
        "id": "4XR9cQGp-a1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:44:02.958209Z",
          "iopub.execute_input": "2024-04-12T19:44:02.959047Z",
          "iopub.status.idle": "2024-04-12T20:01:23.406138Z",
          "shell.execute_reply.started": "2024-04-12T19:44:02.959009Z",
          "shell.execute_reply": "2024-04-12T20:01:23.405231Z"
        },
        "trusted": true,
        "id": "P4rEpy6G57NU",
        "outputId": "8bb3d35f-0d38-4917-b140-e9b6c022023c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n Epoch 1 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 0.547\n\n Epoch 2 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 0.605\n\n Epoch 3 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 0.651\n\n Epoch 4 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 0.722\n\n Epoch 5 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 0.829\n\n Epoch 6 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 0.949\n\n Epoch 7 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 1.058\n\n Epoch 8 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 1.169\n\n Epoch 9 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 1.282\n\n Epoch 10 / 10\n  Batch    50  of    602.\n  Batch   100  of    602.\n  Batch   150  of    602.\n  Batch   200  of    602.\n  Batch   250  of    602.\n  Batch   300  of    602.\n  Batch   350  of    602.\n  Batch   400  of    602.\n  Batch   450  of    602.\n  Batch   500  of    602.\n  Batch   550  of    602.\n  Batch   600  of    602.\n\nEvaluating...\n  Batch    50  of    129.\n  Batch   100  of    129.\n\nTraining Loss: 0.000\nValidation Loss: 1.334\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To make predictions, we will first of all load the best model weights which were saved during the training process."
      ],
      "metadata": {
        "id": "CzUUAprz-mC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T20:01:23.407858Z",
          "iopub.execute_input": "2024-04-12T20:01:23.408190Z",
          "iopub.status.idle": "2024-04-12T20:01:23.649887Z",
          "shell.execute_reply.started": "2024-04-12T20:01:23.408162Z",
          "shell.execute_reply": "2024-04-12T20:01:23.648932Z"
        },
        "trusted": true,
        "id": "CqDpNEZb57NU",
        "outputId": "43626ad0-f1a7-459a-8632-1bb9aebf39c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 135,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Predictions of Test Data\n",
        "Once the weights are loaded, we can use the fine-tuned model to make predictions on the test set."
      ],
      "metadata": {
        "id": "NkJHTB-D-g95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  op = model(test_seq.to(device), token_type_ids=None, attention_mask=test_mask.to(device), labels=test_y.to(device))\n",
        "  loss, logits = op.loss, op.logits\n",
        "  preds = logits\n",
        "  preds = preds.detach().cpu().numpy()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T20:01:23.651247Z",
          "iopub.execute_input": "2024-04-12T20:01:23.651526Z",
          "iopub.status.idle": "2024-04-12T20:01:29.246664Z",
          "shell.execute_reply.started": "2024-04-12T20:01:23.651503Z",
          "shell.execute_reply": "2024-04-12T20:01:29.245832Z"
        },
        "trusted": true,
        "id": "67pdvANJ57NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model’s performance\n",
        "classification report of our model is:"
      ],
      "metadata": {
        "id": "9GTRRFuQ-3-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T20:01:29.248448Z",
          "iopub.execute_input": "2024-04-12T20:01:29.248709Z",
          "iopub.status.idle": "2024-04-12T20:01:29.265716Z",
          "shell.execute_reply.started": "2024-04-12T20:01:29.248687Z",
          "shell.execute_reply": "2024-04-12T20:01:29.264600Z"
        },
        "trusted": true,
        "id": "JFYdN3eq57NV",
        "outputId": "ccc0878b-f143-4aa6-8faa-e094e9c30803"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "              precision    recall  f1-score   support\n\n           0       0.71      0.79      0.75      1668\n           1       0.86      0.78      0.82      1288\n           2       0.79      0.74      0.76      1167\n\n    accuracy                           0.77      4123\n   macro avg       0.79      0.77      0.78      4123\nweighted avg       0.78      0.77      0.77      4123\n\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}